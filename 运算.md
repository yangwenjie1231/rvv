在算法（尤其是科学计算、机器学习、图形学、优化等领域）中，常见的**矩阵和向量运算**可以分为以下几大类：

---

## 一、向量基本运算

### 1. **向量加法 / 减法**
- 要求：两个向量维度相同。
- 公式：  
  $$
  \mathbf{c} = \mathbf{a} \pm \mathbf{b},\quad c_i = a_i \pm b_i
  $$
- 应用：位移合成、梯度更新、神经网络反向传播。

### 2. **标量乘法（数乘）**
- 每个分量乘以一个标量：
  $$
  \mathbf{b} = k \cdot \mathbf{a},\quad b_i = k a_i
  $$
- 应用：归一化、权重缩放、特征工程。

### 3. **点积（内积，Dot Product）**
- 结果为标量：
  $$
  \mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^n a_i b_i = \|\mathbf{a}\| \|\mathbf{b}\| \cos\theta
  $$
- 应用：
  - 计算夹角或相似度（如余弦相似度）
  - 投影计算
  - 神经网络前向传播（$ \mathbf{w}^\top \mathbf{x} $）

### 4. **叉积（外积，Cross Product）**
- 仅适用于三维向量，结果是垂直于原两向量的新向量。
- 应用：计算机图形学中的法向量计算、物理中的力矩。

### 5. **向量范数（Norm）**
- L2 范数（欧几里得长度）：  
  $$
  \|\mathbf{a}\|_2 = \sqrt{\sum a_i^2}
  $$
- L1 范数（曼哈顿距离）：  
  $$
  \|\mathbf{a}\|_1 = \sum |a_i|
  $$
- 应用：正则化（L1/L2）、归一化、误差度量。

### 6. **向量归一化**
- 将向量缩放为单位长度：
  $$
  \hat{\mathbf{a}} = \frac{\mathbf{a}}{\|\mathbf{a}\|}
  $$
- 应用：方向提取、避免数值溢出、相似度计算。

---

## 二、矩阵基本运算

### 1. **矩阵加法 / 减法**
- 要求：同型矩阵（行数列数一致）。
- 对应元素相加/减。

### 2. **标量乘法**
- 所有元素乘以一个常数。

### 3. **矩阵乘法（Matrix Multiplication）**
- 要求：A 的列数 = B 的行数。
- 公式：
  $$
  (AB)_{ij} = \sum_{k=1}^n A_{ik} B_{kj}
  $$
- **不满足交换律**（一般 $ AB \ne BA $），但满足结合律和分配律。
- 应用：
  - 线性变换组合（如旋转+缩放）
  - 神经网络层间传播
  - 马尔可夫链状态转移

### 4. **转置（Transpose）**
- 行列互换：$ (A^\top)_{ij} = A_{ji} $
- 性质：$ (AB)^\top = B^\top A^\top $

### 5. **逆矩阵（Inverse）**
- 仅对方阵定义，且要求可逆（行列式 ≠ 0）。
- 满足：$ A A^{-1} = I $
- 应用：解线性方程组 $ A\mathbf{x} = \mathbf{b} \Rightarrow \mathbf{x} = A^{-1}\mathbf{b} $

### 6. **行列式（Determinant）**
- 仅对方阵定义，记作 $ \det(A) $ 或 $ |A| $
- 几何意义：线性变换的体积缩放因子
- 判定可逆性：$ \det(A) \ne 0 \iff A $ 可逆

### 7. **迹（Trace）**
- 方阵主对角线元素之和：$ \operatorname{tr}(A) = \sum A_{ii} $
- 性质：$ \operatorname{tr}(AB) = \operatorname{tr}(BA) $
- 应用：特征值之和、量子力学期望值

---

## 三、高级矩阵运算（常用于算法优化）

### 1. **矩阵分解**
- **LU 分解**：$ A = LU $，用于高效解线性方程组
- **QR 分解**：$ A = QR $，用于最小二乘、特征值计算
- **特征分解（谱分解）**：$ A = PDP^{-1} $，用于 PCA、动力系统
- **奇异值分解（SVD）**：$ A = U\Sigma V^\top $，用于降维、推荐系统、图像压缩

### 2. **矩阵幂与快速幂**
- $ A^k $：用于图论（邻接矩阵的 k 步可达性）、马尔可夫链稳态分析
- 快速幂算法可在 $ O(\log k) $ 时间内计算（配合矩阵乘法）

### 3. **稀疏矩阵运算**
- 大多数元素为零时，使用 CSR/CSC 格式存储，节省内存并加速计算
- 应用：大规模图算法、自然语言处理（词袋模型）

---

## 四、矩阵与向量的混合运算

### 1. **矩阵 × 向量（核心操作）**
- $ \mathbf{y} = A \mathbf{x} $
- 三种理解视角：
  - **元素视角**：逐行计算点积
  - **行视角**：每行与向量做点积 → 输出每个分量
  - **列视角**：向量作为系数，对矩阵列向量做**线性组合**

> 例如：  
> $$
> A\mathbf{x} = x_1 \mathbf{a}_1 + x_2 \mathbf{a}_2 + \cdots + x_n \mathbf{a}_n
> $$  
> 其中 $ \mathbf{a}_i $ 是 A 的第 i 列。

- 应用：线性回归、坐标变换、状态转移

---

## 五、实际算法中的典型场景

| 领域 | 常用运算 |
|------|--------|
| **机器学习** | 矩阵乘法（前向传播）、SVD（PCA）、协方差矩阵、正规方程 $ (X^\top X)^{-1}X^\top y $ |
| **计算机图形学** | 4×4 变换矩阵（平移、旋转、投影）、向量叉积（光照计算） |
| **优化算法** | Hessian 矩阵（二阶导）、梯度向量、共轭梯度法 |
| **图神经网络** | 邻接矩阵 × 特征矩阵（消息传递） |
| **自然语言处理** | 词嵌入矩阵、注意力机制（QKᵀ 点积） |

---

## 六、数值计算注意事项

- **条件数**：衡量矩阵求解的稳定性，$ \kappa(A) = \|A\| \cdot \|A^{-1}\| $
- **数值稳定性**：避免直接求逆，优先用 QR 或 SVD
- **并行加速**：矩阵运算天然适合 GPU/SIMD 并行（如 cuBLAS、NumPy）

---
